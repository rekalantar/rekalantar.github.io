<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="2.3">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id>
<journal-title>Frontiers in Oncology</journal-title>
<abbrev-journal-title abbrev-type="pubmed">Front. Oncol.</abbrev-journal-title>
<issn pub-type="epub">2234-943X</issn>
<publisher>
<publisher-name>Frontiers Media S.A.</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3389/fonc.2021.665807</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Oncology</subject>
<subj-group>
<subject>Original Research</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>CT-Based Pelvic T<sub>1</sub>-Weighted MR Image Synthesis Using UNet, UNet++ and Cycle-Consistent Generative Adversarial Network (Cycle-GAN)</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kalantar</surname>
<given-names>Reza</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/1223195"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Messiou</surname>
<given-names>Christina</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/719342"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Winfield</surname>
<given-names>Jessica M.</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/664494"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Renn</surname>
<given-names>Alexandra</given-names>
</name>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Latifoltojar</surname>
<given-names>Arash</given-names>
</name>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Downey</surname>
<given-names>Kate</given-names>
</name>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sohaib</surname>
<given-names>Aslam</given-names>
</name>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lalondrelle</surname>
<given-names>Susan</given-names>
</name>
<xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/1169620"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Koh</surname>
<given-names>Dow-Mu</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/1366210"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Blackledge</surname>
<given-names>Matthew D.</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<xref ref-type="author-notes" rid="fn001">
<sup>*</sup>
</xref>
<uri xlink:href="https://loop.frontiersin.org/people/478082"/>
</contrib>
</contrib-group>
<aff id="aff1">
<sup>1</sup>
<institution>Division of Radiotherapy and Imaging, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country>
</aff>
<aff id="aff2">
<sup>2</sup>
<institution>Department of Radiology, The Royal Marsden Hospital</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country>
</aff>
<aff id="aff3">
<sup>3</sup>
<institution>Gynaecological Unit, The Royal Marsden Hospital</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country>
</aff>
<author-notes>
<fn fn-type="edited-by">
<p>Edited by: Oliver Diaz, University of Barcelona, Spain</p>
</fn>
<fn fn-type="edited-by">
<p>Reviewed by: Guang Yang, Imperial College London, United Kingdom; Xiaoran Chen, ETH Z&#xfc;rich, Switzerland</p>
</fn>
<fn fn-type="corresp" id="fn001">
<p>*Correspondence: Matthew D. Blackledge, <email xlink:href="mailto:Matthew.Blackledge@icr.ac.uk">Matthew.Blackledge@icr.ac.uk</email>
</p>
</fn>
<fn fn-type="other" id="fn002">
<p>This article was submitted to Cancer Imaging and Image-directed Interventions, a section of the journal Frontiers in Oncology</p>
</fn>
</author-notes>
<pub-date pub-type="epub">
<day>30</day>
<month>07</month>
<year>2021</year>
</pub-date>
<pub-date pub-type="collection">
<year>2021</year>
</pub-date>
<volume>11</volume>
<elocation-id>665807</elocation-id>
<history>
<date date-type="received">
<day>09</day>
<month>02</month>
<year>2021</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>07</month>
<year>2021</year>
</date>
</history>
<permissions>
<copyright-statement>Copyright &#xa9; 2021 Kalantar, Messiou, Winfield, Renn, Latifoltojar, Downey, Sohaib, Lalondrelle, Koh and Blackledge</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>Kalantar, Messiou, Winfield, Renn, Latifoltojar, Downey, Sohaib, Lalondrelle, Koh and Blackledge</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/">
<p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
</license>
</permissions>
<abstract>
<sec>
<title>Background</title>
<p>Computed tomography (CT) and magnetic resonance imaging (MRI) are the mainstay imaging modalities in radiotherapy planning. In MR-Linac treatment, manual annotation of organs-at-risk (OARs) and clinical volumes requires a significant clinician interaction and is a major challenge. Currently, there is a lack of available pre-annotated MRI data for training supervised segmentation algorithms. This study aimed to develop a deep learning (DL)-based framework to synthesize pelvic T<sub>1</sub>-weighted MRI from a pre-existing repository of clinical planning CTs.</p>
</sec>
<sec>
<title>Methods</title>
<p>MRI synthesis was performed using UNet++ and cycle-consistent generative adversarial network (Cycle-GAN), and the predictions were compared qualitatively and quantitatively against a baseline UNet model using pixel-wise and perceptual loss functions. Additionally, the Cycle-GAN predictions were evaluated through qualitative expert testing (4 radiologists), and a pelvic bone segmentation routine based on a UNet architecture was trained on synthetic MRI using CT-propagated contours and subsequently tested on real pelvic T<sub>1</sub> weighted MRI scans.</p>
</sec>
<sec>
<title>Results</title>
<p>In our experiments, Cycle-GAN generated sharp images for all pelvic slices whilst UNet and UNet++ predictions suffered from poorer spatial resolution within deformable soft-tissues (e.g. bladder, bowel). Qualitative radiologist assessment showed inter-expert variabilities in the test scores; each of the four radiologists correctly identified images as acquired/synthetic with 67%, 100%, 86% and 94% accuracy. Unsupervised segmentation of pelvic bone on T1-weighted images was successful in a number of test cases</p>
</sec>
<sec>
<title>Conclusion</title>
<p>Pelvic MRI synthesis is a challenging task due to the absence of soft-tissue contrast on CT. Our study showed the potential of deep learning models for synthesizing realistic MR images from CT, and transferring cross-domain knowledge which may help to expand training datasets for 21 development of MR-only segmentation models.</p>
</sec>
</abstract>
<kwd-group>
<kwd>convolutional neural network (CNN)</kwd>
<kwd>generative adversarial network (GAN)</kwd>
<kwd>medical image synthesis</kwd>
<kwd>radiotherapy planning</kwd>
<kwd>magnetic resonance imaging (MRI)</kwd>
<kwd>computed tomography (CT)</kwd>
</kwd-group>
<contract-sponsor id="cn001">NIHR Biomedical Research Centre, Royal Marsden NHS Foundation Trust/Institute of Cancer Research<named-content content-type="fundref-id">10.13039/100014461</named-content>
</contract-sponsor>
<counts>
<fig-count count="7"/>
<table-count count="3"/>
<equation-count count="2"/>
<ref-count count="26"/>
<page-count count="12"/>
<word-count count="3902"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Computed tomography (CT) is conventionally used for the delineation of the gross tumor volume (GTV) and subsequent clinical/planning target volumes (CTV/PTV), along with organs-at-risk (OARs) in radiotherapy (RT) treatment planning. Resultant contours allow optimization of treatment plans by delivering the required dose to PTVs whilst minimizing radiation exposure of the OARs by ensuring that spatial dose constraints are not exceeded. Magnetic resonance imaging (MRI) offers excellent soft-tissue contrast and is generally used in conjunction with CT to improve visualization of the GTV and surrounding OARs during treatment planning. However, manual definition of these regions is repetitive, cumbersome and may be subject to inter- and/or intra-operator variabilities (<xref ref-type="bibr" rid="B1">1</xref>). The recent development of the combined MR-Linac system (<xref ref-type="bibr" rid="B2">2</xref>) provides the potential for accurate treatment adaption through online MR-imaging acquired prior to each RT fraction. However, re-definition of contours for each MR-Linac treatment fraction requires approximately 10 minutes of downtime whilst the patient remains on the scanner bed, placing additional capacity pressures on clinicians wishing to adopt this technology.</p>
<p>Deep learning (DL) is a sub-category of artificial intelligence (AI), inspired by the human cognition system. In contrast to traditional machine learning approaches that use hand-engineered image-processing routines, DL is able to learn complex information from large datasets. In recent years, DL-based approaches have shown great promise in medical imaging applications, including image synthesis (<xref ref-type="bibr" rid="B3">3</xref>, <xref ref-type="bibr" rid="B4">4</xref>) and automatic segmentation (<xref ref-type="bibr" rid="B5">5</xref>&#x2013;<xref ref-type="bibr" rid="B7">7</xref>). There is great promise for DL to drastically accelerate delineation of the GTV and OARs in MR-Linac studies, yet a major hurdle remains the lack of large existing pre-contoured MRI datasets for training supervised segmentation networks. One potential solution is transferring knowledge from pre-existing RT planning repositories on CT to MRI in order to facilitate domain adaptive segmentation (<xref ref-type="bibr" rid="B8">8</xref>). Previous studies have reported successful implementation of GANs in generating realistic CT images from MRI (<xref ref-type="bibr" rid="B3">3</xref>, <xref ref-type="bibr" rid="B9">9</xref>&#x2013;<xref ref-type="bibr" rid="B11">11</xref>) as well as MRI synthesis from CT in the brain (<xref ref-type="bibr" rid="B12">12</xref>). To date, few studies have investigated MRI synthesis in the pelvis. Dong et al. (<xref ref-type="bibr" rid="B13">13</xref>) proposed a synthetic MRI-assisted framework for improved multi-organ segmentation on CT. However, although the authors suggested that synthetic MR images improved segmentation results, the quality of synthesis was not investigated in depth. MR image synthesis from CT is a challenging task due to large soft-tissue signal intensity variations. In particular, MRI synthesis in the pelvis offers the considerable difficulty posed by geometrical differences in patient anatomy as well as unpredictable discrepancies in bladder and bowel contents.</p>
<p>In this study, we compare and contrast paired and unpaired generative techniques for synthesizing T<sub>1</sub>-weighted (T<sub>1</sub>W) MR images from pelvic CT scans as a basis for training algorithms for OARs and tumor delineation on acquired MRI datasets. We include in our analysis the use of state-of-the-art UNet (<xref ref-type="bibr" rid="B14">14</xref>) and UNet++ (<xref ref-type="bibr" rid="B15">15</xref>) architectures for paired training, testing two different loss functions [L<sub>1</sub> and VGG-19 perceptual loss (<xref ref-type="bibr" rid="B16">16</xref>)], and compare our results with a Cycle-Consistent Generative Adversarial Network (Cycle-GAN) (<xref ref-type="bibr" rid="B17">17</xref>) for unpaired MR image synthesis. Subsequently, we evaluate our results through blinded assessment of synthetic and acquired images by expert radiologists, and demonstrate our approach for pelvic bone segmentation on acquired T<sub>1</sub>W MRI from a framework trained solely on synthetic <sub>1</sub>W MR images with CT-propagated contours.</p>
</sec>
<sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2_1">
<title>Patient Population and Imaging Protocols</title>
<p>Our cohort consisted of 26 patients with lymphoma who underwent routine PET/CT scanning (Gemini, Philips, Cambridge, United States) and whole-body T<sub>1</sub>W MRI (1.5T, Avanto, Siemens Healthcare, Erlangen, Germany) before and after treatment (see <xref ref-type="table" rid="T1">
<bold>Table&#xa0;1</bold>
</xref> for imaging protocols). From this cohort, image series with large axial slice angle mismatch between CT and MR images, and those from patients with metal implants were excluded, leaving 28 paired CT/MRI datasets from 17 patients. The studies involving human participants were reviewed and approved by the Committee for Clinical Research at the Royal Marsden Hospital. The patients/participants provided written informed consent to participate in this study.</p>
<table-wrap id="T1" position="float">
<label>Table&#xa0;1</label>
<caption>
<p>Imaging parameters for acquired CT and T<sub>1</sub>W MR images.</p>
</caption>
<table frame="hsides">
<thead>
<tr>
<th valign="top" colspan="2" align="left">CT parameters</th>
<th valign="top" colspan="2" align="center">T<sub>1</sub>W MR parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">Peak Voltage Output (kVp)</td>
<td valign="top" align="center">120</td>
<td valign="top" align="left">Acquisition Sequence</td>
<td valign="top" align="center">2D Spoiled Gradient Echo</td>
</tr>
<tr>
<td valign="top" align="left">Acquisition Type</td>
<td valign="top" align="center">Helical</td>
<td valign="top" align="left">Echo Time (ms)</td>
<td valign="top" align="center">4.8</td>
</tr>
<tr>
<td valign="top" align="left">Slice Thickness (mm)</td>
<td valign="top" align="center">3-6.5</td>
<td valign="top" align="left">Repetition Time (ms)</td>
<td valign="top" align="center">386</td>
</tr>
<tr>
<td valign="top" align="left">Matrix Size</td>
<td valign="top" align="center">512 <italic>&#xd7;</italic> 512</td>
<td valign="top" align="left">Phase Encoding Direction</td>
<td valign="top" align="center">Anterior-Posterior</td>
</tr>
<tr>
<td valign="top" align="left">Pixel Spacing (mm<sup>2</sup>)</td>
<td valign="top" align="center">0.74 <italic>&#xd7;</italic> 0.74-1.17 <italic>&#xd7;</italic> 1.17</td>
<td valign="top" align="left">Acquired Matrix Size (read)</td>
<td valign="top" align="center">256</td>
</tr>
<tr>
<td valign="top" align="left">Exposure (mAs)</td>
<td valign="top" align="center">26-80</td>
<td valign="top" align="left">Reconstructed Matrix Size (read)</td>
<td valign="top" align="center">512</td>
</tr>
<tr>
<td valign="top" rowspan="7" align="left"/>
<td valign="top" rowspan="7" align="center"/>
<td valign="top" align="left">Reconstructed Pixel Size (mm<sup>2</sup>)</td>
<td valign="top" align="center">0.74 <italic>&#xd7;</italic> 0.74-0.82 <italic>&#xd7;</italic> 0.82</td>
</tr>
<tr>
<td valign="top" align="left">Flip Angle</td>
<td valign="top" align="center">70&#xb0;</td>
</tr>
<tr>
<td valign="top" align="left">Slice Orientation</td>
<td valign="top" align="center">Axial</td>
</tr>
<tr>
<td valign="top" align="left">Slice Thickness (mm)</td>
<td valign="top" align="center">5</td>
</tr>
<tr>
<td valign="top" align="left">Acceleration</td>
<td valign="top" align="center">GRAPPA (R=2)</td>
</tr>
<tr>
<td valign="top" align="left">Bandwidth</td>
<td valign="top" align="center">Pixel</td>
</tr>
<tr>
<td valign="top" align="left">(Hz/pixel)</td>
<td valign="top" align="center">331</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>Some parameters are shown as the range of values (minimum-maximum) existing in the patient datasets.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="s2_2">
<title>Model Architectures</title>
<p>We investigated three DL architectures for MR image synthesis: (i) UNet, (ii) UNet++, and (iii) Cycle-GAN. UNet is one of the most popular DL architectures for image-to-image translations, with initial applications in image segmentation (<xref ref-type="bibr" rid="B14">14</xref>). In essence, UNet is an auto-encoder with addition of skip connections between encoding and decoding sections to maintain spatial resolution. In this study, a baseline UNet model was designed consisting of 10 consecutive convolutional blocks (5 encoding and 5 decoding blocks), each using batch normalization and ReLU activation for CT-to-MR image generation (<xref ref-type="fig" rid="f1">
<bold>Figure&#xa0;1A</bold>
</xref>). Additionally, a UNet++ model with interconnected skip connection pathways, as described in (<xref ref-type="bibr" rid="B15">15</xref>), was developed with the same number of encoder-decoder sections and kernel filters as the baseline UNet (<xref ref-type="fig" rid="f1">
<bold>Figure&#xa0;1B</bold>
</xref>). UNet++ was reported to enhance performance (<xref ref-type="bibr" rid="B15">15</xref>), therefore we deployed this architecture to assess its capabilities for paired image synthesis.</p>
<fig id="f1" position="float">
<label>Figure&#xa0;1</label>
<caption>
<p>Paired image-to-image networks, <bold>(A)</bold> UNet with symmetrical skip connections between the encoder and decoder, <bold>(B)</bold> UNet++ with interconnected skip connection convolutional pathways.</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g001.tif"/>
</fig>
<p>GANs are the state-of-the-art approaches for generating photo-realistic images based on the principles of game theory (<xref ref-type="bibr" rid="B18">18</xref>). In image synthesis applications, GANs typically consist of two CNNs, a generator and a discriminator. During training, the generator produces a target synthetic image from an input image with different modality; the discriminator then attempts to classify whether the synthetic image is genuine. Training is successful once the generator is able to synthesize images that the discriminator is unable to differentiate from real examples. Progressive co-training of the generator and discriminator leads to learning of the global conditional probability distribution from input to target domain. In this study, a Cycle-GAN model (<xref ref-type="bibr" rid="B17">17</xref>) was developed to facilitate unpaired CT-to-MR and MR-to-CT learning. The baseline UNet model was used as the network generator, and the discriminator composed of 5 blocks containing 2D convolutional layers followed by instance normalization and leaky ReLU activation. This technique offers the advantage that it does not require spatial alignment between training T<sub>1</sub>W MR and CT images. The high-level schematic of the Cycle-GAN network is shown in <xref ref-type="fig" rid="f2">
<bold>Figure&#xa0;2</bold>
</xref>.</p>
<fig id="f2" position="float">
<label>Figure&#xa0;2</label>
<caption>
<p>Schematic of the Cycle-GAN network. During training, images from CT domain are translated to MRI domain and reconstructed back to CT domain under the governance of adversarial and cycle consistency loss terms respectively. Co-training of CT-to-MRI and MRI-to-CT models leads to generation of photo-realistic predictions.</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g002.tif"/>
</fig>
<p>For segmentation, we propose a framework that first generates synthetic T1W MR images from CT, propagates ground-truth CT contours and outputs segmentation contours on acquired T1W MR images. To examine the capability of our fully-automated DL framework for knowledge transfer from CT to MRI, we generate ground-truth contours of the bones using a Gaussian mixture model proposed by Blackledge etal. (<xref ref-type="bibr" rid="B19">19</xref>) and transfer them to synthetic MR images as a basis for our segmentation training. A similar UNet model to the architecture presented in <xref ref-type="fig" rid="f1">
<bold>Figure&#xa0;1</bold>
</xref>, with 5 convolutional blocks (convolution-batchnorm-dropout(p=0.5)-ReLU) in the encoding and decoding sections was developed to perform binary bone segmentation from synthetic MR images. The schematic of our proposed synthesis/segmentation framework is illustrated in <xref ref-type="fig" rid="f3">
<bold>Figure&#xa0;3</bold>
</xref>.</p>
<fig id="f3" position="float">
<label>Figure&#xa0;3</label>
<caption>
<p>Schematic of the proposed fully-automatic combined synthesis and segmentation framework for knowledge transfer from CT scans to MR images. The intermediate synthesis stage enables segmentation training using CT-based contours and MR signal distributions.</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g003.tif"/>
</fig>
</sec>
<sec id="s2_3">
<title>Image Preprocessing</title>
<p>In preparation for paired training, the corresponding CT and T<sub>1</sub>W MR slices from the anatomical pelvic station in each patient were resampled using a 2D affine transformation followed by non-rigid registration using multi-resolution B-Spline free-form deformation (loss = Mattes mutual information, histogram bins = 50, gradient descent line search optimizer parameters: learning rate = 5.0, number of iterations = 50, convergence window size = 10) (<xref ref-type="bibr" rid="B20">20</xref>). The resulting co-registered images were visually qualified based on the alignment of rigid pelvic landmarks. In CT images, signal intensities outside of the range -1000 and 1000 HU were truncated to limit the dynamic range. The T<sub>1</sub>W MR images were corrected using N4 bias-field correction to reduce inter-patient intensity variations and inhomogeneities (<xref ref-type="bibr" rid="B21">21</xref>) and signal intensities above 1500 (corresponding to infrequent high intensity fatty regions) were truncated. Subsequently, the training images were normalized to intensity ranges (0,1) and (-1,1) prior to paired (UNet, UNet++) and unpaired (Cycle-GAN) training respectively.</p>
</sec>
<sec id="s2_4">
<title>Objective Functions</title>
<p>Common loss functions in image synthesis are mean absolute error (MAE or L<sub>1</sub>) and mean squared error (MSE or L<sub>2</sub>) between the target domain and the synthetic output. However, such loss functions ignore complex image features such as texture and shape. Therefore, for UNet/UNet++ models, we compared L<sub>1</sub> loss in the image space with L<sub>1</sub> loss calculated based on the features extracted from a previously-trained object classification network, deriving the &#x201c;perceptual loss&#x201d;. For this purpose, the VGG-19 classification network was used (<xref ref-type="bibr" rid="B16">16</xref>), which is composed of 5 convolutional layers and 19 layers overall, and used features extracted from the &#x201c;block conv2d&#x201d; layer. For Cycle-GAN training, the difference between L<sub>1</sub> and the structural similarity index (SSIM) (defined as L<sub>1</sub> &#x2013; SSIM) was used as the loss to govern the cycle consistency, whilst L<sub>1</sub> and L<sub>2</sub> losses were used for the generator and the discriminator respectively. For segmentation training, the Dice loss (1, 2) was used to perform binary division of bone on MR images.</p>
<disp-formula>
<label>(1)</label>
<mml:math display="block" id="M1">
<mml:mrow>
<mml:mi>D</mml:mi>
<mml:mi>S</mml:mi>
<mml:mi>C</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mo>|</mml:mo>
<mml:mi>A</mml:mi>
<mml:mo>&#x2229;</mml:mo>
<mml:mi>B</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:mi>A</mml:mi>
<mml:mo>|</mml:mo>
<mml:mo>+</mml:mo>
<mml:mo>|</mml:mo>
<mml:mi>B</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
</disp-formula>
<disp-formula>
<label>(2)</label>
<mml:math display="block" id="M2">
<mml:mrow>
<mml:mi mathvariant="italic">Dice&#x2009;loss</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>&#x2212;</mml:mo>
<mml:mi mathvariant="italic">DSC</mml:mi>
</mml:mrow>
</mml:math>
</disp-formula>
<p>where A and B denote the generated and ground-truth contours.</p>
</sec>
<sec id="s2_5">
<title>Model Training and Evaluation</title>
<p>The dataset was split to 981, 150 and 116 images from 11, 3 and 3 patients for training, validation and testing respectively. All models were trained for 150 epochs using the Adam optimizer (learning rate = 1e-4; UNet and UNet++ models: batch size = 5, Cycle-GAN: batch size=1) on a NVIDIA RTX6000 GPU (Santa Clara, California, United States) (<xref ref-type="table" rid="T2">
<bold>Table&#xa0;2</bold>
</xref>). During paired UNet/UNet++ training, the peak signal-to-noise ratio (PSNR), SSIM, L<sub>1</sub> and L<sub>2</sub> quantitative metrics, as described in (<xref ref-type="bibr" rid="B22">22</xref>), were recorded at each epoch for the validation images. The trained weights with the lowest validation loss were used to generate synthetic T<sub>1</sub>W MR images from the test CT images. Optimal weights from the Cycle-GAN model were selected based on visual examination of the network predictions of the validation data following each epoch. Subsequently, synthetic images from all models were evaluated against the ground-truth acquired MR images quantitatively using the above-mentioned imaging metrics. An additional qualitative test was designed to obtain unbiased clinical examination of predictions from the Cycle-GAN model. This test consisted of two sections: (i) to blindly classify randomly-selected test images as synthetic or acquired, and outline reasoning for answers (18 synthetic and 18 acquired test MR slices), and (ii) to describe key differences between synthetic and acquired test T<sub>1</sub>W MR images when the input CT and ground truth acquired MR images were also provided (10 sets of images from 3 test patients). This test was completed by 4 radiologists (two with <italic>&lt;</italic>5 years and two with <italic>&gt;</italic>5 years of experience). The segmentation network was trained on Cycle-GAN generated synthetic MR images (training: 14, validation: 3 patients) for 600 epochs using the Adam optimizer (learning rate = 1e-4) and batch size of 1. To avoid overfitting, random linear shear and rotation (range:0, <italic>&#x3c0;</italic>/60) were applied to images during training.</p>
<table-wrap id="T2" position="float">
<label>Table&#xa0;2</label>
<caption>
<p>Learnable parameters (in millions) of UNet, UNet++ and Cycle-GAN models.</p>
</caption>
<table frame="hsides">
<thead>
<tr>
<th valign="top" align="left"/>
<th valign="top" align="center">UNet (L<sub>1</sub>)</th>
<th valign="top" align="center">UNet (VGG)</th>
<th valign="top" align="center">UNet++ (L<sub>1</sub>)</th>
<th valign="top" align="center">Cycle-GAN</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">Trainable Parameters (M)</td>
<td valign="top" align="center">31</td>
<td valign="top" align="center">31</td>
<td valign="top" align="center">36</td>
<td valign="top" align="center">31(G), 11(D)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>G and D denote the Cycle-GAN generator and discriminator respectively.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>Results</title>
<p>Quantitative assessment of synthetic T<sub>1</sub>W MR images from the validation dataset during paired algorithm training suggested that the UNet and UNet++ models with L<sub>1</sub> loss displayed higher PSNR and SSIM, and lower L<sub>1</sub> and L<sub>2</sub> values compared with the generated images from the UNet model with the VGG-19 perceptual loss (<xref ref-type="fig" rid="f4">
<bold>Figure&#xa0;4</bold>
</xref>). Quantitative analysis of synthetic images from the test patients revealed a similar trend for UNet and UNet++ model predictions and showed that the Cycle-GAN quantitative values were the lowest in all metrics but the SSIM where it was only higher than UNet (VGG) predictions (<xref ref-type="table" rid="T3">
<bold>Table&#xa0;3</bold>
</xref>). Moreover, qualitative evaluation of predictions from all models revealed a noticeable difference in sharpness (spatial resolution) between the images generated from paired (UNet and UNet++) and unpaired (Cycle-GAN) training. It was observed that despite UNet and UNet++ models generating relatively realistic predictions for pelvic slices consisting of fixed and bony structures (e.g. femoral heads, hip bone, muscles), they yielded blurry and unrealistic patches for deformable and variable pelvic structures such as bowel, bladder and rectum. In contrast, the Cycle-GAN model generated sharp images for all pelvic slices, yet a disparity in contrast was observed for soft-tissues with large variabilities in training patient MRI slices (e.g. bowel content, gas in rectum and bowel, bladder filling) (<xref ref-type="fig" rid="f5">
<bold>Figure&#xa0;5</bold>
</xref>).</p>
<fig id="f4" position="float">
<label>Figure&#xa0;4</label>
<caption>
<p>Quantitative metrics calculated from validation images during training of UNet and UNet++ models for 150 epochs. <bold>(A)</bold> PSNR, <bold>(B)</bold> SSIM, <bold>(C)</bold> L<sub>1</sub> loss and <bold>(D)</bold> L<sub>2</sub> loss.</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g004.tif"/>
</fig>
<table-wrap id="T3" position="float">
<label>Table&#xa0;3</label>
<caption>
<p>Quantitative analysis of predictions from the trained models on test patients.</p>
</caption>
<table frame="hsides">
<thead>
<tr>
<th valign="top" align="left">&#xa0;</th>
<th valign="top" align="center">UNet (L<sub>1</sub>)</th>
<th valign="top" align="center">UNet (VGG)</th>
<th valign="top" align="center">UNet++ (L<sub>1</sub>)</th>
<th valign="top" align="center">Cycle-GAN PSNR</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="top" align="left">
<bold>PSNR</bold>
</td>
<td valign="top" align="center">
<bold>20</bold>.<bold>169</bold> <italic>&#xb1;</italic> <bold>0</bold>.<bold>196</bold>
</td>
<td valign="top" align="center">19.668 <italic>&#xb1;</italic> 0.189</td>
<td valign="top" align="center">20.080 <italic>&#xb1;</italic> 0.193</td>
<td valign="top" align="char" char="&#xb1;">18.279 <italic>&#xb1;</italic> 0.156</td>
</tr>
<tr>
<td valign="top" align="left">
<bold>SSIM</bold>
</td>
<td valign="top" align="center">
<bold>0</bold>.<bold>809</bold> <italic>&#xb1;</italic> <bold>0</bold>.<bold>003</bold>
</td>
<td valign="top" align="center">0.728 <italic>&#xb1;</italic> 0.003</td>
<td valign="top" align="center">0.804 <italic>&#xb1;</italic> 0.003</td>
<td valign="top" align="char" char="&#xb1;">0.783 <italic>&#xb1;</italic> 0.003</td>
</tr>
<tr>
<td valign="top" align="left">
<bold>MAE</bold>
</td>
<td valign="top" align="center">
<bold>0</bold>.<bold>043</bold> <italic>&#xb1;</italic> <bold>0</bold>.<bold>001</bold>
</td>
<td valign="top" align="center">0.047 <italic>&#xb1;</italic> 0.001</td>
<td valign="top" align="center">0.044 <italic>&#xb1;</italic> 0.001</td>
<td valign="top" align="char" char="&#xb1;">0.057 <italic>&#xb1;</italic> 0.001</td>
</tr>
<tr>
<td valign="top" align="left">
<bold>MSE</bold>
</td>
<td valign="top" align="center">
<bold>0</bold>.<bold>011</bold> <italic>&#xb1;</italic> <bold>0</bold>.<bold>001</bold>
</td>
<td valign="top" align="center">
<bold>0</bold>.<bold>011</bold> <italic>&#xb1;</italic> <bold>0</bold>.<bold>001</bold>
</td>
<td valign="top" align="center">0.013 <italic>&#xb1;</italic> 0.001</td>
<td valign="top" align="char" char="&#xb1;">0.016 <italic>&#xb1;</italic> 0.001</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>The calculated metrics are presented as mean and standard deviation. The pixel intensities outside the body were excluded when deriving these measurements. The best quantitative metrics are shown in bold.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="f5" position="float">
<label>Figure&#xa0;5</label>
<caption>
<p>T<sub>1</sub>T1W MRI predictions generated from 3 independent test patients using UNet, UNet++ and Cycle-GAN models (panel <bold>A</bold>: patient 1, panels <bold>B&#x2013;E</bold>, <bold>G</bold>: patient 2, panel <bold>F</bold>: patient 3). Red box: Predictions from pelvic slices with relatively fixed geometries including the bones demonstrate sharp boundaries between anatomical structures, with visually superior results for the Cycle-GAN architecture (panels <bold>A</bold>, <bold>F</bold>). Green box: The superior resolution of the Cycle-GAN architecture is further exemplified in slices with deformable structures such as the bowel loop (panels <bold>F</bold>, <bold>G</bold>). In highly deformable regions, minor contrast disparity in anatomical structures can be observed in the synthetic MRI; examples include prediction of bladder (red arrows in panel <bold>C</bold>), lower gastrointestinal region (red arrows in panels <bold>D</bold>, <bold>E</bold>) and rectum (blue arrows in panels <bold>C</bold>, <bold>D</bold>).</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g005.tif"/>
</fig>
<p>Our expert radiologist qualitative testing on Cycle-GAN predicted images suggested that there were inter-expert variabilities in scores from section one of the test, highlighting the differences in subjective decisions amongst the experts in a number of test images. Experts 1 and 2 (<italic>&lt;</italic>5 years of experience) scored 67% and 100% whilst experts 3 and 4 (<italic>&gt;</italic>5 years of experience) correctly identified 86% and 94% of total 36 test images. Hence, no particular correlation was observed between the percentage scores and the participants&#x2019; years of experience (<xref ref-type="fig" rid="f6">
<bold>Figure&#xa0;6A</bold>
</xref>). Radiologist comments on the synthetic images (following unblinding) are presented in <xref ref-type="fig" rid="f6">
<bold>Figure&#xa0;6B</bold>
</xref>.</p>
<fig id="f6" position="float">
<label>Figure&#xa0;6</label>
<caption>
<p>
<bold>(A)</bold> Section One: Expert scores for identifying evenly-distributed test patient MRI slices as synthetic or acquired, <bold>(B)</bold> Section Two: Expert comments on Cycle-GAN synthetic MRI when presented along with the ground truth CT and acquired T<sub>1</sub>W MRI (Experts 1 and 2 with &lt;5 years of experience, and experts 3 and 4 with &gt;5 years of experience).</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g006.tif"/>
</fig>
<p>The bone segmentation results using our fully-automated approach showed that our proposed framework successfully performed unsupervised segmentation of the bone from acquired T<sub>1</sub>W MR images, without the requirement of any manually annotated regions of interest (ROIs). The outcome from various pelvic slices across 8 patients from our in-house cohort are presented in <xref ref-type="fig" rid="f7">
<bold>Figure&#xa0;7</bold>
</xref>. The segmentation results from cases 5 to 8 were from patients not used in the synthesis and segmentation components of our framework. Test case 8 demonstrates the predicted bone contours from a patient with metal hip implant.</p>
<fig id="f7" position="float">
<label>Figure&#xa0;7</label>
<caption>
<p>Bone segmentation results from acquired T<sub>1</sub>W MRI scans of 8 test patients using the proposed fully-automated framework. The combined synthesis/segmentation network allows transfer of organ- specific encoded spatial information from CT to MRI without the need to manually define ROIs. Cases 5 to 8 were patients not included in the synthesis stage of network training. Case 8 shows bone segmentation results from a patient with metal hip.</p>
</caption>
<graphic mimetype="image" mime-subtype="tiff" xlink:href="fonc-11-665807-g007.tif"/>
</fig>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion and Conclusion</title>
<p>One major limitation in adaptive RT on the MR-Linac system is the need for manual annotation of OARs and tumors on patient scans for each RT fraction which requires significant clinician interaction. DL-based approaches are promising solutions to automate this task and reduce burden on clinicians. However, the development of these algorithms is hindered by the paucity of pre-annotated MRI datasets for training and validation. In this study, we developed paired and unpaired training for T<sub>1</sub>W MR image synthesis from pelvic CT scans as a data generative tool for training of segmentation algorithms for MR-Linac RT treatment planning. Our results suggested that the Cycle-GAN network generated synthetic images with the greatest visual fidelity across all pelvic slices whilst the synthetic images from UNet and UNet++ appeared less sharp, which is likely due to soft-tissue misalignments during the registration process. The observed disparity in contrast in Cycle-GAN images for bladder, bone marrow and bowel loops may be due to large variabilities in our relatively small training dataset. Although the direct impact of these contrast discrepancies on MRI segmentation performance is yet to be evaluated, the Cycle-GAN predictions appeared more suitable for CT contour propagation to synthetic MRI than UNet and UNet++ images due to distinctive soft-tissue boundaries and high-resolution synthesis.</p>
<p>Quantitative analysis of all model predictions indicated that the imaging metrics did not fully conform with the output image visual fidelity and apparent sharpness. This finding was in fact in line previous studies comparing paired and unpaired MRI synthesis (<xref ref-type="bibr" rid="B12">12</xref>, <xref ref-type="bibr" rid="B22">22</xref>). CT-to-MR synthesis in the pelvis offers the considerable challenge of generating soft-tissue contrasts absent on acquired CT scans. Although quantitative metrics such as the PSNR, SSIM, L<sub>1</sub> and L<sub>2</sub> differences are useful measures when comparing images, they may not directly correspond to photo-realistic network outcome. This was evident in quantitative evaluation of the images generated from the UNet and UNet++ models trained with L<sub>1</sub> loss in the image space against UNet with VGG-19 perceptual loss and Cycle-GAN predictions. Therefore, expert clinician qualitative assessments may provide a more reliable insight into the performance of medical image generative networks. In this study, our expert evaluation test based on Cycle-GAN predictions suggested that despite a number of suboptimal soft-tissue contrast predictions (e.g. urinary bladder filling, bone marrow, nerves), there were differences in radiologist accuracies for correctly identifying synthetic from acquired MR images. The fact that 3/4 radiologists were unable to accurately identify synthetic images in all cases highlights the capability of our model to generate realistic medical images that may be indistinguishable from acquired MRI.</p>
<p>Our segmentation results demonstrated the capability of our fully-automated framework in segmenting bones on acquired MRI images with no manual MR contouring. Domain adaptation offers a significant clinical value in transferring knowledge from previously-contoured OARs by experts on CT to MR-only treatment planning procedures. Additionally, it potentially enables expanding medical datasets which are essential for training supervised DL models. Such a technique is also highly valuable outside the context of radiotherapy, as body MRI has increasing utility for monitoring patients with secondary bone disease from primary prostate (<xref ref-type="bibr" rid="B23">23</xref>) and breast (<xref ref-type="bibr" rid="B24">24</xref>) cancers, and multiple myeloma (<xref ref-type="bibr" rid="B25">25</xref>). Quantitative assessment of response of these diseases to systemic treatment using MRI is hindered by the lack of automated skeletal delineation algorithms to monitor changes in large volume disease regions (<xref ref-type="bibr" rid="B26">26</xref>).</p>
<p>GANs are notoriously difficult to train due to their large degree of application-based hyper-parameter optimization and non-standardized training techniques. However, this study showed that even when trained on relatively small datasets, GANs may have the potential to generate realistic images to overcome the challenge of medical image data shortage. Therefore, fut ure studies will investigate the performance of the proposed framework on larger datasets and alternative pelvic OARs, as well as exploring novel techniques to enforce targeted organ contrast during GAN and segmentation training. Additionally, future research will examine the performance sensitivity on the level of manual MRI contours required for training cross-domain DL algorithms.</p>
</sec>
<sec id="s5" sec-type="data-availability">
<title>Data Availability Statement</title>
<p>The data analyzed in this study is subject to the following licenses/restrictions: The datasets presented in this article are not readily available due to patient confidentiality concerns. Requests to access these datasets should be directed to <email xlink:href="mailto:matthew.Blackledge@icr.ac.uk">matthew.Blackledge@icr.ac.uk</email>.</p>
</sec>
<sec id="s6">
<title>Ethics Statement</title>
<p>The studies involving human participants were reviewed and approved by the Committee for Clinical Research at the Royal Marsden Hospital. The patients/participants provided written informed consent to participate in this study.</p>
</sec>
<sec id="s7">
<title>Author Contributions</title>
<p>All authors listed have made a substantial, direct, and intellectual contribution to the work, and approved it for publication.</p>
</sec>
<sec id="s8" sec-type="funding-information">
<title>Funding</title>
<p>This project represents independent research funded by the National Institute for Health Research (NIHR) Biomedical Research Centre at The Royal Marsden NHS Foundation Trust and the Institute of Cancer Research, London, United Kingdom.</p>
</sec>
<sec id="s9">
<title>Author Disclaimer</title>
<p>The views expressed are those of the authors and not necessarily those of the NIHR or the Department of Health and Social Care.</p>
</sec>
<sec id="s10" sec-type="COI-statement">
<title>Conflict of Interest</title>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
</sec>
<sec id="s11" sec-type="disclaimer">
<title>Publisher&#x2019;s Note</title>
<p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="B1">
<label>1</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Millioni</surname> <given-names>R</given-names>
</name>
<name>
<surname>Sbrignadello</surname> <given-names>S</given-names>
</name>
<name>
<surname>Tura</surname> <given-names>A</given-names>
</name>
<name>
<surname>Iori</surname> <given-names>E</given-names>
</name>
<name>
<surname>Murphy</surname> <given-names>E</given-names>
</name>
<name>
<surname>Tessari</surname> <given-names>P</given-names>
</name>
</person-group>. <article-title>The Inter- and Intra-Operator Variability in Manual Spot Segmentation and its Effect on Spot Quantitation in Two-Dimensional Electrophoresis Analysis</article-title>. <source>Electrophoresis</source> (<year>2010</year>) <volume>31</volume>:<page-range>1739&#x2013;42</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1002/elps.200900674</pub-id>
</citation>
</ref>
<ref id="B2">
<label>2</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Raaymakers</surname> <given-names>B</given-names>
</name>
<name>
<surname>Lagendijk</surname> <given-names>J</given-names>
</name>
<name>
<surname>Overweg</surname> <given-names>J</given-names>
</name>
<name>
<surname>Kok</surname> <given-names>J</given-names>
</name>
<name>
<surname>Raaijmakers</surname> <given-names>A</given-names>
</name>
<name>
<surname>Kerkhof</surname> <given-names>E</given-names>
</name>
<etal/>
</person-group>. <article-title>Integratinga 1.5&#xa0;T Mri Scanner With a 6 Mv Accelerator: Proof of Concept</article-title>. <source>Phys Med Biol</source> (<year>2009</year>) <volume>54</volume>:<fpage>N229</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1088/0031-9155/54/12/N01</pub-id>
</citation>
</ref>
<ref id="B3">
<label>3</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Wolterink</surname> <given-names>JM</given-names>
</name>
<name>
<surname>Dinkla</surname> <given-names>AM</given-names>
</name>
<name>
<surname>Savenije</surname> <given-names>MH</given-names>
</name>
<name>
<surname>Seevinck</surname> <given-names>PR</given-names>
</name>
<name>
<surname>van den Berg</surname> <given-names>CA</given-names>
</name>
<name>
<surname>Is&#x2c7;gum</surname> <given-names>I</given-names>
</name>
</person-group>. &#x201c;<article-title>Deep Mr to Ct Synthesis Using Unpaired Data</article-title>&#x201d;. In: <source>International Workshop on Simulation and Synthesis in Medical Imaging</source>. <publisher-name>Springer, Cham </publisher-name> (<year>2017</year>). p. <fpage>14</fpage>&#x2013;<lpage>23</lpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1007/978-3-319-68127-6_2</pub-id>
</citation>
</ref>
<ref id="B4">
<label>4</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Nie</surname> <given-names>D</given-names>
</name>
<name>
<surname>Trullo</surname> <given-names>R</given-names>
</name>
<name>
<surname>Lian</surname> <given-names>J</given-names>
</name>
<name>
<surname>Petitjean</surname> <given-names>C</given-names>
</name>
<name>
<surname>Ruan</surname> <given-names>S</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>Q</given-names>
</name>
<etal/>
</person-group>. &#x201c;<article-title>Medical Image Synthesis With Context-Aware Generative Adversarial Networks</article-title>&#x201d;. In: <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2017</year>). p. <page-range>417&#x2013;25</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1007/978-3-319-66179-7_48</pub-id>
</citation>
</ref>
<ref id="B5">
<label>5</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Song</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Hu</surname> <given-names>J</given-names>
</name>
<name>
<surname>Wu</surname> <given-names>Q</given-names>
</name>
<name>
<surname>Xu</surname> <given-names>F</given-names>
</name>
<name>
<surname>Nie</surname> <given-names>S</given-names>
</name>
<name>
<surname>Zhao</surname> <given-names>Y</given-names>
</name>
<etal/>
</person-group>. <article-title>Automatic Delineation of the Clinical Target Volume and Organs at Risk by Deep Learning for Rectal Cancer Postoperative Radiotherapy</article-title>. <source>Radiother Oncol</source> (<year>2020</year>) <volume>145</volume>:<page-range>186&#x2013;92</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.radonc.2020.01.020</pub-id>
</citation>
</ref>
<ref id="B6">
<label>6</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Boon</surname> <given-names>IS</given-names>
</name>
<name>
<surname>Au Yong</surname> <given-names>T</given-names>
</name>
<name>
<surname>Boon</surname> <given-names>CS</given-names>
</name>
</person-group>. <article-title>Assessing the Role of Artificial Intelligence (Ai) in Clinical Oncology: Utility of Machine Learning in Radiotherapy Target Volume Delineation</article-title>. <source>Medicines</source> (<year>2018</year>) <volume>5</volume>:<fpage>131</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.3390/medicines5040131</pub-id>
</citation>
</ref>
<ref id="B7">
<label>7</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lustberg</surname> <given-names>T</given-names>
</name>
<name>
<surname>van Soest</surname> <given-names>J</given-names>
</name>
<name>
<surname>Gooding</surname> <given-names>M</given-names>
</name>
<name>
<surname>Peressutti</surname> <given-names>D</given-names>
</name>
<name>
<surname>Aljabar</surname> <given-names>P</given-names>
</name>
<name>
<surname>van der Stoep</surname> <given-names>J</given-names>
</name>
<etal/>
</person-group>. <article-title>Clinical Evaluation of Atlas and Deep Learning Based Automatic Contouring for Lung Cancer</article-title>. <source>Radiother Oncol</source> (<year>2018</year>) <volume>126</volume>:<page-range>312&#x2013;7</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.radonc.2017.11.012</pub-id>
</citation>
</ref>
<ref id="B8">
<label>8</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Masoudi</surname> <given-names>S</given-names>
</name>
<name>
<surname>Harmon</surname> <given-names>S</given-names>
</name>
<name>
<surname>Mehralivand</surname> <given-names>S</given-names>
</name>
<name>
<surname>Walker</surname> <given-names>S</given-names>
</name>
<name>
<surname>Ning</surname> <given-names>H</given-names>
</name>
<name>
<surname>Choyke</surname> <given-names>P</given-names>
</name>
<etal/>
</person-group>. <article-title>Cross Modality Domain Adaptation to Generate Pelvic Magnetic Resonance Images From Computed Tomography Simulation for More Accurate Prostate Delineation in Radiotherapy Treatment</article-title>. <source>Int J Radiat Oncol Biol Phys</source> (<year>2020</year>) <volume>108</volume>:<fpage>e924</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.ijrobp.2020.07.572</pub-id>
</citation>
</ref>
<ref id="B9">
<label>9</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Yang</surname> <given-names>H</given-names>
</name>
<name>
<surname>Sun</surname> <given-names>J</given-names>
</name>
<name>
<surname>Carass</surname> <given-names>A</given-names>
</name>
<name>
<surname>Zhao</surname> <given-names>C</given-names>
</name>
<name>
<surname>Lee</surname> <given-names>J</given-names>
</name>
<name>
<surname>Xu</surname> <given-names>Z</given-names>
</name>
<etal/>
</person-group>. &#x201c;<article-title>Unpaired Brain Mr-to-Ct Synthesis Using a Structure-Constrained Cyclegan</article-title>&#x201d;. In: <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source>. <publisher-name>Springer</publisher-name> (<year>2018</year>). p. <page-range>174&#x2013;82</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1007/978-3-030-00889-5_20</pub-id>
</citation>
</ref>
<ref id="B10">
<label>10</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lei</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Harms</surname> <given-names>J</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>T</given-names>
</name>
<name>
<surname>Liu</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Shu</surname> <given-names>H-K</given-names>
</name>
<name>
<surname>Jani</surname> <given-names>AB</given-names>
</name>
<etal/>
</person-group>. <article-title>Mri-Only Based Synthetic Ct Generation Using Dense Cycle Consistent Generative Adversarial Networks</article-title>. <source>Med Phys</source> (<year>2019</year>) <volume>46</volume>:<page-range>3565&#x2013;81</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1002/mp.13617</pub-id>
</citation>
</ref>
<ref id="B11">
<label>11</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liang</surname> <given-names>X</given-names>
</name>
<name>
<surname>Chen</surname> <given-names>L</given-names>
</name>
<name>
<surname>Nguyen</surname> <given-names>D</given-names>
</name>
<name>
<surname>Zhou</surname> <given-names>Z</given-names>
</name>
<name>
<surname>Gu</surname> <given-names>X</given-names>
</name>
<name>
<surname>Yang</surname> <given-names>M</given-names>
</name>
<etal/>
</person-group>. <article-title>Generating Synthesized Computed Tomography (Ct) From Cone-Beam Computed Tomography (CBCT) Using Cyclegan for Adaptive Radiation Therapy</article-title>. <source>Phys Med Biol</source> (<year>2019</year>) <volume>64</volume>:<fpage>125002</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1088/1361-6560/ab22f9</pub-id>
</citation>
</ref>
<ref id="B12">
<label>12</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jin</surname> <given-names>C-B</given-names>
</name>
<name>
<surname>Kim</surname> <given-names>H</given-names>
</name>
<name>
<surname>Liu</surname> <given-names>M</given-names>
</name>
<name>
<surname>Jung</surname> <given-names>W</given-names>
</name>
<name>
<surname>Joo</surname> <given-names>S</given-names>
</name>
<name>
<surname>Park</surname> <given-names>E</given-names>
</name>
<etal/>
</person-group>. <article-title>Deep Ct to Mr Synthesis Using Paired and Unpaired Data</article-title>. <source>Sensors</source> (<year>2019</year>) <volume>19</volume>:<fpage>2361</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.3390/s19102361</pub-id>
</citation>
</ref>
<ref id="B13">
<label>13</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dong</surname> <given-names>X</given-names>
</name>
<name>
<surname>Lei</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Tian</surname> <given-names>S</given-names>
</name>
<name>
<surname>Wang</surname> <given-names>T</given-names>
</name>
<name>
<surname>Patel</surname> <given-names>P</given-names>
</name>
<name>
<surname>Curran</surname> <given-names>WJ</given-names>
</name>
<etal/>
</person-group>. <article-title>Synthetic Mri-Aided Multi-Organ Segmentation on Male Pelvic Ct Using Cycle Consistent Deep Attention Network</article-title>. <source>Radiother Oncol</source> (<year>2019</year>) <volume>141</volume>:<page-range>192&#x2013;9</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.radonc.2019.09.028</pub-id>
</citation>
</ref>
<ref id="B14">
<label>14</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ronneberger</surname> <given-names>O</given-names>
</name>
<name>
<surname>Fischer</surname> <given-names>P</given-names>
</name>
<name>
<surname>Brox</surname> <given-names>T</given-names>
</name>
</person-group>. &#x201c;<article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title>&#x201d;. In: <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2015</year>). p. <page-range>234&#x2013;41</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
</citation>
</ref>
<ref id="B15">
<label>15</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhou</surname> <given-names>Z</given-names>
</name>
<name>
<surname>Siddiquee</surname> <given-names>MMR</given-names>
</name>
<name>
<surname>Tajbakhsh</surname> <given-names>N</given-names>
</name>
<name>
<surname>Liang</surname> <given-names>J</given-names>
</name>
</person-group>. <article-title>Unet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</article-title>. <source>IEEE Trans Med Imaging</source> (<year>2019</year>) <volume>39</volume>:<page-range>1856&#x2013;67</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id>
</citation>
</ref>
<ref id="B16">
<label>16</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simonyan</surname> <given-names>K</given-names>
</name>
<name>
<surname>Zisserman</surname> <given-names>A</given-names>
</name>
</person-group>. <article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title>. (<year>2014</year>) arXiv preprint arXiv:1409.1556.</citation>
</ref>
<ref id="B17">
<label>17</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zhu</surname> <given-names>J-Y</given-names>
</name>
<name>
<surname>Park</surname> <given-names>T</given-names>
</name>
<name>
<surname>Isola</surname> <given-names>P</given-names>
</name>
<name>
<surname>Efros</surname> <given-names>AA</given-names>
</name>
</person-group>. <article-title>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</article-title>. In: <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<year>2017</year>) p. <page-range>2223&#x2013;32</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1109/ICCV.2017.244</pub-id>
</citation>
</ref>
<ref id="B18">
<label>18</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Goodfellow</surname> <given-names>I</given-names>
</name>
<name>
<surname>Pouget-Abadie</surname> <given-names>J</given-names>
</name>
<name>
<surname>Mirza</surname> <given-names>M</given-names>
</name>
<name>
<surname>Xu</surname> <given-names>B</given-names>
</name>
<name>
<surname>Warde-Farley</surname> <given-names>D</given-names>
</name>
<name>
<surname>Ozair</surname> <given-names>S</given-names>
</name>
<etal/>
</person-group>. &#x201c;<article-title>Generative Adversarial Nets</article-title>&#x201d;. In: <source>Advances in Neural Information Processing Systems</source> (<year>2014</year>). p. <page-range>2672&#x2013;80</page-range>.</citation>
</ref>
<ref id="B19">
<label>19</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blackledge</surname> <given-names>MD</given-names>
</name>
<name>
<surname>Collins</surname> <given-names>DJ</given-names>
</name>
<name>
<surname>Koh</surname> <given-names>D-M</given-names>
</name>
<name>
<surname>Leach</surname> <given-names>MO</given-names>
</name>
</person-group>. <article-title>Rapid Development of Image Analysis Research Tools: Bridging the Gap Between Researcher and Clinician With Pyosirix</article-title>. <source>Comput Biol Med</source> (<year>2016</year>) <volume>69</volume>:<page-range>203&#x2013;12</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.compbiomed.2015.12.002</pub-id>
</citation>
</ref>
<ref id="B20">
<label>20</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keszei</surname> <given-names>AP</given-names>
</name>
<name>
<surname>Berkels</surname> <given-names>B</given-names>
</name>
<name>
<surname>Deserno</surname> <given-names>TM</given-names>
</name>
</person-group>. <article-title>Survey of Non-Rigid Registration Tools in Medicine</article-title>. <source>J Digital Imaging</source> (<year>2017</year>) <volume>30</volume>:<page-range>102&#x2013;16</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1007/s10278-016-9915-8</pub-id>
</citation>
</ref>
<ref id="B21">
<label>21</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Tustison</surname> <given-names>NJ</given-names>
</name>
<name>
<surname>Avants</surname> <given-names>BB</given-names>
</name>
<name>
<surname>Cook</surname> <given-names>PA</given-names>
</name>
<name>
<surname>Zheng</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Egan</surname> <given-names>A</given-names>
</name>
<name>
<surname>Yushkevich</surname> <given-names>PA</given-names>
</name>
<etal/>
</person-group>. &#x201c;<article-title>N4itk: Improved N3 Bias Correction</article-title>&#x201d;. In: <source>IEEE Transactions on Medical Imaging</source>. (<year>2010</year>) p. <volume>29</volume>:<page-range>1310&#x2013;20</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id>
</citation>
</ref>
<ref id="B22">
<label>22</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname> <given-names>W</given-names>
</name>
<name>
<surname>Li</surname> <given-names>Y</given-names>
</name>
<name>
<surname>Qin</surname> <given-names>W</given-names>
</name>
<name>
<surname>Liang</surname> <given-names>X</given-names>
</name>
<name>
<surname>Xu</surname> <given-names>J</given-names>
</name>
<name>
<surname>Xiong</surname> <given-names>J</given-names>
</name>
<etal/>
</person-group>. <article-title>Magnetic Resonance Image (Mri) Synthesis From Brain Computed Tomography (Ct) Images Based on Deep Learning Methods for Magnetic Resonance (Mr)-Guided Radiotherapy</article-title>. <source>Quant Imaging Med Surg</source> (<year>2020</year>) <volume>10</volume>:<fpage>1223</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.21037/qims-19-885</pub-id>
</citation>
</ref>
<ref id="B23">
<label>23</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Padhani</surname> <given-names>AR</given-names>
</name>
<name>
<surname>Lecouvet</surname> <given-names>FE</given-names>
</name>
<name>
<surname>Tunariu</surname> <given-names>N</given-names>
</name>
<name>
<surname>Koh</surname> <given-names>D-M</given-names>
</name>
<name>
<surname>De Keyzer</surname> <given-names>F</given-names>
</name>
<name>
<surname>Collins</surname> <given-names>DJ</given-names>
</name>
<etal/>
</person-group>. <article-title>Metastasis Reporting and Data System for Prostate Cancer: Practical Guidelines for Acquisition, Interpretation, and Reporting of Whole-Body Magnetic Resonance Imaging-Based Evaluations of Multiorgan Involvement in Advanced Prostate Cancer</article-title>. <source>Eur Urol</source> (<year>2017</year>) <volume>71</volume>:<fpage>81</fpage>&#x2013;<lpage>92</lpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1016/j.eururo.2016.05.033</pub-id>
</citation>
</ref>
<ref id="B24">
<label>24</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Padhani</surname> <given-names>AR</given-names>
</name>
<name>
<surname>Koh</surname> <given-names>D-M</given-names>
</name>
<name>
<surname>Collins</surname> <given-names>DJ</given-names>
</name>
</person-group>. <article-title>Whole-Body Diffusion-Weighted Mr Imaging in Cancer: Current Status and Research Directions</article-title>. <source>Radiology</source> (<year>2011</year>) <volume>261</volume>:<page-range>700&#x2013;18</page-range>. doi:&#xa0;<pub-id pub-id-type="doi">10.1148/radiol.11110474</pub-id>
</citation>
</ref>
<ref id="B25">
<label>25</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Messiou</surname> <given-names>C</given-names>
</name>
<name>
<surname>Hillengass</surname> <given-names>J</given-names>
</name>
<name>
<surname>Delorme</surname> <given-names>S</given-names>
</name>
<name>
<surname>Lecouvet</surname> <given-names>FE</given-names>
</name>
<name>
<surname>Moulopoulos</surname> <given-names>LA</given-names>
</name>
<name>
<surname>Collins</surname> <given-names>DJ</given-names>
</name>
<etal/>
</person-group>. <article-title>Guidelines for Acquisition, Interpretation, and Reporting of Whole-Body Mri in Myeloma: Myeloma Response Assessment and Diagnosis System (My-Rads)</article-title>. <source>Radiology</source> (<year>2019</year>) <volume>291</volume>:<fpage>5</fpage>&#x2013;<lpage>13</lpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1148/radiol.2019181949</pub-id>
</citation>
</ref>
<ref id="B26">
<label>26</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blackledge</surname> <given-names>MD</given-names>
</name>
<name>
<surname>Collins</surname> <given-names>DJ</given-names>
</name>
<name>
<surname>Tunariu</surname> <given-names>N</given-names>
</name>
<name>
<surname>Orton</surname> <given-names>MR</given-names>
</name>
<name>
<surname>Padhani</surname> <given-names>AR</given-names>
</name>
<name>
<surname>Leach</surname> <given-names>MO</given-names>
</name>
<etal/>
</person-group>. <article-title>Assessment of Treatment Response by Total Tumor Volume and Global Apparent Diffusion Coefficient Using Diffusion-Weighted MRI in Patients With Metastatic Bone Disease: A Feasibility Study</article-title>. <source>PLoS One</source> (<year>2014</year>) <volume>9</volume>:<fpage>e91779</fpage>. doi:&#xa0;<pub-id pub-id-type="doi">10.1371/journal.pone.0091779</pub-id>
</citation>
</ref>
</ref-list>
</back>
</article>