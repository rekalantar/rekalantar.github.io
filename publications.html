<!DOCTYPE html>
<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Publications, Google Scholar, Conferences, ​Kalantar, R., Winfield, J.M., Messiou, C., Lalondrelle, S., Koh, D.M. and Blackledge, M., 2022. Organs-at-Risk Segmentation on T2-Weighted Magnetic Resonance Imaging Using a Transformer-Based Model. International Journal of Radiation Oncology, Biology, Physics, 114(3), pp.e554-e555. ASTRO, 2022.​, ​Rieu, R.,&nbsp;Kalantar, R., Yu, S., Koh, D.M., Lalondrelle, S. and Blackledge, M., 2022. Prediction of Patients at Risk of Pelvic Insufficiency Fractures Following Pelvic Radiotherapy. International Journal of Radiation Oncology, Biology, Physics, 114(3), pp.e124-e125. ASTRO, 2022., ​Kalantar, R., Lalondrelle, S., Winfield, J.M., Messiou, C., Koh, D.M. and Blackledge, M.D., Synthetic MRI-assisted Multi-Wavelet Segmentation Framework for Organs-at-Risk Delineation on CT for Radiotherapy Planning. ISMRM, 2021., ​Kalantar, R., Winfield, J.M., Messiou, C., Koh, D.M. and Blackledge, M.D., CT-based Synthetic pelvic T1-weighted MR image generation using a deep convolutional neural network (CNN). ISMRM, 2020.">
    <meta name="description" content="">
    <title>Publications</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Publications.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 5.3.2, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    <link id="u-page-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400i,700,700i">




    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": ""
}</script>
    <meta name="theme-color" content="#ff5126">
    <meta property="og:title" content="Publications">
    <meta property="og:type" content="website">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-container-align-center u-custom-color-5 u-header u-sticky u-sticky-38fd" id="sec-0a44" data-animation-name="" data-animation-duration="0" data-animation-delay="0" data-animation-direction=""><div class="u-clearfix u-sheet u-sheet-1">
        <nav class="u-align-center u-menu u-menu-one-level u-offcanvas u-menu-1">
          <div class="menu-collapse u-custom-font" style="font-size: 1.125rem; letter-spacing: 0px; font-family: &quot;PT Serif&quot;;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-text-active-color u-custom-text-color u-custom-text-hover-color u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-custom-font u-nav u-spacing-20 u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/index.html" style="padding: 10px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/blogs.html" style="padding: 10px;">Blogs</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/publications.html" style="padding: 10px;">Publications</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/education.html" style="padding: 10px;">Education</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/awards.html" style="padding: 10px;">Awards</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-3-light-1 u-text-grey-5 u-text-hover-palette-2-dark-1" href="/about.html" style="padding: 10px;">About</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-60 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-spacing-7 u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/index.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/blogs.html">Blogs</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/publications.html">Publications</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/education.html">Education</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/awards.html">Awards</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="/about.html">About</a>
</li></ul>
              </div>
            </div>
            <div class="u-menu-overlay"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-clearfix u-container-align-center u-image u-section-1" id="sec-89a4" data-image-width="1920" data-image-height="768">
      <div class="u-clearfix u-sheet u-sheet-1"></div>
    </section>
    <section class="u-align-center u-clearfix u-grey-10 u-section-2" id="sec-8e8d">
      <div class="u-clearfix u-sheet u-valign-middle-xs u-sheet-1">
        <h2 class="u-custom-font u-text u-text-1">Publications</h2>
        <h2 class="u-custom-font u-text u-text-default u-text-palette-2-dark-1 u-text-2">
          <a href="https://scholar.google.com/citations?user=d2zZmXMAAAAJ&amp;hl=en" class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-hover-palette-3-light-1 u-text-palette-2-dark-1 u-btn-1">Google Scholar</a>
        </h2>
        <div class="u-accordion u-expanded-width u-faq u-spacing-10 u-accordion-1">
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-6" id="link-accordion-new3" aria-controls="accordion-new3" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Ingle, M., Rieu, R., Curcean, S., Winfield, J.M., Lin, G., Messiou, C., Lalondrelle, S., Koh, D.M. and Blackledge, M.D., 2025. Domain-Adaptive and Per-Fraction Guided Deep Learning Framework for Magnetic Resonance Imaging-Based Segmentation of Organs at Risk in Gynecologic Cancers. Advances in Radiation Oncology, 10(4), p.101745.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-6"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-new3"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-new3" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-6" id="accordion-new3" aria-labelledby="link-accordion-new3">
              <div class="u-container-layout u-container-layout-6">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">This study proposes a domain-adaptive and per-fraction guided deep learning framework for magnetic resonance imaging-based segmentation of organs at risk in gynecologic cancers. The framework demonstrates significant improvements in segmentation accuracy and robustness, paving the way for enhanced radiotherapy planning and treatment outcomes.</span>
                  </p>
                  <p>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://doi.org/10.1016/j.adro.2025.101745">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-7" id="link-accordion-new4" aria-controls="accordion-new4" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Lin, G., Winfield, J.M., Messiou, C., Koh, D.M. and Blackledge, M.D., 2023, November. MED-INPAINT: Medical Image Synthesis Using Multi-Level Conditional Inpainting with a Denoising Diffusion Probabilistic Model and Adaptive Contrast Priors. In 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI) (pp. 403-413). IEEE.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-7"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-new4"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-new4" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-7" id="accordion-new4" aria-labelledby="link-accordion-new4">
              <div class="u-container-layout u-container-layout-7">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">This paper introduces MED-INPAINT, a novel framework for medical image synthesis using multi-level conditional inpainting with a denoising diffusion probabilistic model and adaptive contrast priors. The proposed method demonstrates significant improvements in generating high-quality synthetic medical images, enabling enhanced diagnostic and research applications.</span>
                  </p>
                  <p>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://doi.org/10.1109/MedAI.2023.403413">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-5" id="link-accordion-new2" aria-controls="accordion-new2" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Curcean, S., Winfield, J.M., Lin, G., Messiou, C., Blackledge, M.D. and Koh, D.M., 2023. Deep learning framework with multi-head dilated encoders for enhanced segmentation of cervical cancer on multiparametric magnetic resonance imaging. Diagnostics, 13(21), p.3381.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-5"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-new2"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-new2" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-5" id="accordion-new2" aria-labelledby="link-accordion-new2">
              <div class="u-container-layout u-container-layout-5">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">This study introduces a deep learning framework with multi-head dilated encoders to enhance segmentation of cervical cancer on multiparametric magnetic resonance imaging. The proposed method demonstrates improved accuracy and robustness in delineating cervical cancer regions, offering potential for better clinical decision-making and treatment planning.</span>
                  </p>
                  <p>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://doi.org/10.3390/diagnostics13213381">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-4" id="link-accordion-new" aria-controls="accordion-new" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Hindocha, S., Hunter, B., Sharma, B., Khan, N., Koh, D.M., Ahmed, M., Aboagye, E.O., Lee, R.W. and Blackledge, M.D., 2023. Non-contrast CT synthesis using patch-based cycle-consistent generative adversarial network (Cycle-GAN) for radiomics and deep learning in the era of COVID-19. Scientific Reports, 13(1), p.10568.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-new"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-new" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-4" id="accordion-new" aria-labelledby="link-accordion-new">
              <div class="u-container-layout u-container-layout-4">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">This study presents a novel approach for non-contrast CT synthesis using a patch-based cycle-consistent generative adversarial network (Cycle-GAN). The method is evaluated for its potential applications in radiomics and deep learning, particularly in the context of the COVID-19 pandemic. The results demonstrate the feasibility of generating high-quality synthetic CT images, which can aid in reducing the dependency on contrast agents and improving diagnostic workflows.</span>
                  </p>
                  <p>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://doi.org/10.1038/s41598-023-10568-0">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-2" id="link-accordion-72f4" aria-controls="accordion-72f4" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Messiou, C., Winfield, J.M., Renn, A., Latifoltojar, A., Downey, K., Sohaib, A., Lalondrelle, S., Koh, D.M. and Blackledge, M.D., 2021. CT-Based Pelvic T1-Weighted MR Image Synthesis Using UNet, UNet++ and Cycle-Consistent Generative Adversarial Network (Cycle-GAN). Frontiers in Oncology, 11, p.665807.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-05a3"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-05a3" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-2" id="accordion-72f4" aria-labelledby="link-accordion-72f4">
              <div class="u-container-layout u-container-layout-2">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">Background: Computed tomography (CT) and magnetic resonance imaging (MRI) are the mainstay imaging modalities in radiotherapy planning. In MR-Linac treatment, manual annotation of organs-at-risk (OARs) and clinical volumes requires a significant clinician interaction and is a major challenge. Currently, there is a lack of available pre-annotated MRI data for training supervised segmentation algorithms. This study aimed to develop a deep learning (DL)-based framework to synthesize pelvic T1-weighted MRI from a pre-existing repository of clinical planning CTs.</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">Methods:&nbsp;MRI synthesis was performed using UNet++ and cycle-consistent generative adversarial network (Cycle-GAN), and the predictions were compared qualitatively and quantitatively against a baseline UNet model using pixel-wise and perceptual loss functions. Additionally, the Cycle-GAN predictions were evaluated through qualitative expert testing (4 radiologists), and a pelvic bone segmentation routine based on a UNet architecture was trained on synthetic MRI using CT-propagated contours and subsequently tested on real pelvic T1&nbsp;weighted MRI scans.</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">Results:&nbsp;In our experiments, Cycle-GAN generated sharp images for all pelvic slices whilst UNet and UNet++ predictions suffered from poorer spatial resolution within deformable soft-tissues (e.g. bladder, bowel). Qualitative radiologist assessment showed inter-expert variabilities in the test scores; each of the four radiologists correctly identified images as acquired/synthetic with 67%, 100%, 86% and 94% accuracy. Unsupervised segmentation of pelvic bone on T1-weighted images was successful in a number of test cases.</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">Conclusion: Pelvic MRI synthesis is a challenging task due to the absence of soft-tissue contrast on CT. Our study showed the potential of deep learning models for synthesizing realistic MR images from CT, and transferring cross-domain knowledge which may help to expand training datasets for 21 development of MR-only segmentation models.</span>
                    <span style="font-size: 1rem;">&nbsp;</span>
                    <span class="u-text-palette-2-dark-1" style="font-size: 1rem;">
                      <a href="https://pubmed.ncbi.nlm.nih.gov/34395244/"></a>
                    </span>
                    <span style="font-size: 1rem;">
                      <span class="u-text-palette-2-dark-1">
                        <a href="https://pubmed.ncbi.nlm.nih.gov/34395244/">[pdf]</a>
                      </span>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-3" id="link-accordion-854e" aria-controls="accordion-854e" aria-selected="false">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Vaid, S., and Bhandari, M., 2020. Deep learning COVID-19 detection bias: accuracy through artificial intelligence. International Orthopaedics, 44(8), pp.1539-1542.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-4a3a"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-4a3a" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-pane u-container-style u-white u-accordion-pane-3" id="accordion-854e" aria-labelledby="link-accordion-854e">
              <div class="u-container-layout u-container-layout-3">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                    <span style="font-size: 1rem;">This study explores the potential biases in deep learning models for COVID-19 detection. Using chest X-ray scans and convolutional neural networks (CNNs), the model achieved 96.3% accuracy, highlighting its ability to minimize false positives and negatives.</span>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://link.springer.com/article/10.1007/s00264-020-04609-7">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
          <div class="u-accordion-item">
            <a class="active u-accordion-link u-active-white u-button-style u-custom-font u-hover-white u-text-grey-90 u-white u-accordion-link-1" id="link-accordion-f600" aria-controls="accordion-f600" aria-selected="true">
              <span class="u-accordion-link-text"><b>Kalantar, R.</b>, Lin, G., Winfield, J.M., Messiou, C., Lalondrelle, S., Blackledge, M.D., and Koh, D.M., 2021. Automatic segmentation of pelvic cancers using deep learning: state-of-the-art approaches and challenges. Diagnostics, 11(11), p.1964.
              </span><span class="u-accordion-link-icon u-icon u-text-grey-50 u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 448 448" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-3ae2"></use></svg><svg class="u-svg-content" viewBox="0 0 448 448" id="svg-3ae2" style=""><path d="m272 184c-4.417969 0-8-3.582031-8-8v-176h-80v176c0 4.417969-3.582031 8-8 8h-176v80h176c4.417969 0 8 3.582031 8 8v176h80v-176c0-4.417969 3.582031-8 8-8h176v-80zm0 0"></path></svg></span>
            </a>
            <div class="u-accordion-active u-accordion-pane u-container-style u-white u-accordion-pane-1" id="accordion-f600" aria-labelledby="link-accordion-f600">
              <div class="u-container-layout u-container-layout-1">
                <div class="fr-view u-clearfix u-rich-text u-text">
                  <p>
                    <span style="font-weight: 700; font-size: 1rem;">Abstract:&nbsp;</span>
                  </p>
                  <p>
                    <span style="font-size: 1rem;">This review highlights the challenges and advancements in deep learning-based segmentation of pelvic cancers, focusing on bladder, prostate, cervical, and rectal cancers using CT and MRI imaging.</span>
                    <span class="u-text-palette-2-dark-1">
                      <a href="https://pubmed.ncbi.nlm.nih.gov/34829310/">[pdf]</a>
                    </span>
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-align-center u-clearfix u-grey-90 u-section-3" id="carousel_797e">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h2 class="u-custom-font u-text u-text-default u-text-1">Conferences</h2>
        <h2 class="u-align-left u-custom-font u-text u-text-default u-text-2"><b>Kalantar, R.</b>, Winfield, J.M., Messiou, C., Lalondrelle, S., Koh, D.M. and Blackledge, M., 2022. Organs-at-Risk Segmentation on T2-Weighted Magnetic Resonance Imaging Using a Transformer-Based Model. International Journal of Radiation Oncology, Biology, Physics, 114(3), pp.e554-e555. ASTRO, 2022.<span style="font-weight: 400;"></span>
        </h2>
        <h2 class="u-align-left u-custom-font u-text u-text-default u-text-3"> Rieu, R.,&nbsp;<b>Kalantar, R.</b>, Yu, S., Koh, D.M., Lalondrelle, S. and Blackledge, M., 2022. Prediction of Patients at Risk of Pelvic Insufficiency Fractures Following Pelvic Radiotherapy. International Journal of Radiation Oncology, Biology, Physics, 114(3), pp.e124-e125. ASTRO, 2022.
        </h2>
        <h2 class="u-align-left u-custom-font u-text u-text-default u-text-4"><b>Kalantar, R.</b>, Lalondrelle, S., Winfield, J.M., Messiou, C., Koh, D.M. and Blackledge, M.D., Synthetic MRI-assisted Multi-Wavelet Segmentation Framework for Organs-at-Risk Delineation on CT for Radiotherapy Planning. ISMRM, 2021.
        </h2>
        <h2 class="u-align-left u-custom-font u-text u-text-default u-text-5"><b>Kalantar, R.</b>, Winfield, J.M., Messiou, C., Koh, D.M. and Blackledge, M.D., CT-based Synthetic pelvic T1-weighted MR image generation using a deep convolutional neural network (CNN). ISMRM, 2020.
        </h2>
      </div>
    </section>


    <footer class="u-align-center u-clearfix u-footer u-grey-80 u-footer" id="sec-918f"><div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-small-text u-text u-text-variant u-text-1"> © Reza Kalantar. All rights reserved.</p>
      </div></footer>

</body></html>
